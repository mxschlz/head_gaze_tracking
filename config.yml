#======================================================================================
# HeadGazeTracker Configuration File
#
# This file controls all operational parameters for the tracker.
# Sections are organized by function. Read comments carefully before editing.
#======================================================================================

#--------------------------------------------------------------------------------------
# 1. System & Performance Settings
# Controls for processing speed, on-screen display, and video output format.
#--------------------------------------------------------------------------------------
System:
  # If True, prints detailed status updates to the console. Essential for debugging.
  PRINT_DATA: True

  # If True, shows the video window with all on-screen data overlays.
  # Set to False for "headless" batch processing to gain a significant speed boost.
  SHOW_ON_SCREEN_DATA: True

  # --- Performance Tuning ---
  # To improve speed, you can process a subset of frames.
  # 1 = Process every frame (most detailed, slowest).
  # 2 = Process every 2nd frame (faster, good balance).
  # 3+ = Process every Nth frame (fastest, less temporal detail).
  FRAME_SKIP: 1

  # Video encoding format for the output file.
  # "mp4v" is recommended for .mp4 files. "XVID" is good for .avi files.
  OUTPUT_VIDEO_FOURCC: "mp4v"

  # --- Socket Communication (for sending data in real-time) ---
  # Master switch to enable sending data over UDP socket.
  USE_SOCKET: False
  # IP address of the receiving server.
  SERVER_IP: "127.0.0.1"
  # Port of the receiving server.
  SERVER_PORT: 5005

#--------------------------------------------------------------------------------------
# 2. Input & Output File Settings
# Defines video orientation, splitting, and naming conventions for output files.
#--------------------------------------------------------------------------------------
Files:
  # Corrects video orientation if it was recorded sideways or upside down.
  # Options: 0 (no rotation), 90, 180, 270.
  ROTATE: 180
  # Set to True if the video is a mirror image (e.g., from some webcams).
  FLIP_VIDEO: False

  # --- Video Splitting ---
  # If you need to split a long video into two parts for analysis (e.g., pre/post).
  # Time in milliseconds (ms) at which to split the video.
  # Set to null or remove the line to disable splitting.
  SPLIT_VIDEO_AT_MS: 655000 # Example: 655 seconds

  # --- Output Filename Prefixes & Suffixes ---
  # Base name for the main per-frame data log file.
  OUTPUT_MAIN_LOG_FILENAME_PREFIX: "eye_tracking_log"
  # Base name for the trial summary log file.
  OUTPUT_TRIAL_SUMMARY_FILENAME_PREFIX: "trial_summary"
  # Suffix for files generated from the first part of a split video.
  OUTPUT_FILENAME_SUFFIX_PART1: "_part1"
  # Suffix for files generated from the second part of a split video.
  OUTPUT_FILENAME_SUFFIX_PART2: "_part2"

  # --- Logging Settings ---
  # Master switch to enable saving data to CSV files.
  LOG_DATA: True
  # Format for timestamps in the log files (follows Python's strftime format).
  TIMESTAMP_FORMAT: "%Y%m%d%H%M%S%f"
  # If True, logs all 468/478 landmark coordinates. Creates very large files.
  LOG_ALL_FEATURES: False
  # If LOG_ALL_FEATURES is True, also log the Z-coordinate for each landmark.
  LOG_Z_COORD: False

#--------------------------------------------------------------------------------------
# 3. Core Model & Feature Settings
# High-level switches for enabling/disabling major features and models.
#--------------------------------------------------------------------------------------
Features:
  # --- MediaPipe Model Settings ---
  # Maximum number of faces to detect in a frame.
  MAX_NUM_FACES: 1
  # Confidence threshold for the initial face detection. Lower is more lenient.
  MIN_DETECTION_CONFIDENCE: 0.01
  # Confidence threshold for tracking the face across frames. Lower is more lenient.
  MIN_TRACKING_CONFIDENCE: 0.01
  # Use the more detailed (but slower) attention mesh for eyes/lips.
  # Set to False for a performance boost if you only need head pose.
  USE_ATTENTION_MESH: True

  # --- Head Pose ---
  # Master switch to enable or disable all head pose estimation and logging.
  ENABLE_HEAD_POSE: True
  # Width of the user's face in millimeters (outer cheekbone to outer cheekbone).
  # This is important for accurate scaling of the 3D head model.
  USER_FACE_WIDTH: 150 # [mm]
  # Number of frames to average for smoothing head pose angles. Higher = smoother but more lag.
  MOVING_AVERAGE_WINDOW: 10

  # --- Blink Detection ---
  # Eye Aspect Ratio (EAR) threshold. A lower value means the eyes must be more closed to count as a blink.
  BLINK_THRESHOLD: 0.51
  # Number of consecutive frames the EAR must be below the threshold to register a blink.
  EYE_AR_CONSEC_FRAMES: 1

  # --- On-Screen Display ---
  # If True, draws all 468+ facial landmarks. Very slow, for debugging only.
  SHOW_ALL_FEATURES: False

#--------------------------------------------------------------------------------------
# 4. Head Pose Calibration
# Determines the "neutral" or "forward-looking" baseline pose.
#--------------------------------------------------------------------------------------
Calibration:
  # Method to use for calibration.
  # "clustering": (RECOMMENDED FOR INFANTS) A two-pass method that analyzes a period of the video
  #               to find the most common head pose and sets it as the baseline. Most robust.
  # "gaze_informed": A single-pass method that collects samples only when the user's eyes
  #                  are looking straight ahead. Good for cooperative users.
  # "manual": No automatic calibration. You must press 'c' to set the baseline.
  # "none": No calibration is performed. Raw angle values are used.
  METHOD: "clustering"

  # Duration in seconds at the start of the video to use for automatic calibration.
  DURATION_SECONDS: 60

  # --- Settings for "clustering" method ---
  Clustering:
    # Max distance (in degrees) between two head poses to be in the same cluster.
    # INCREASE if no cluster is found. DECREASE for a tighter, more precise baseline.
    DBSCAN_EPS: 3.0
    # Minimum number of frames that must be in a cluster to be considered a valid baseline.
    DBSCAN_MIN_SAMPLES: 15

  # --- Settings for "gaze_informed" method ---
  GazeInformed:
    # Minimum number of valid samples required to complete calibration.
    MIN_SAMPLES: 30
    # Allowable horizontal/vertical deviation (in pixels) of the iris from the eye corner
    # for a frame to be considered "looking forward" and used for calibration.
    EYE_DX_RANGE: [-20, 20]
    EYE_DY_RANGE: [-20, 20]

#--------------------------------------------------------------------------------------
# 5. Trial Detection & Gaze Classification
# Logic for identifying trials and classifying the user's gaze during those trials.
#--------------------------------------------------------------------------------------
Trials:
  # Master switch for the entire trial detection and classification system.
  ENABLE: True

  # --- Trial Onset Detection (via screen brightness) ---
  # Defines the Region of Interest [x, y, width, height] on the video frame to monitor for a stimulus.
  STIMULUS_ROI_COORDS: [820, 630, 300, 300]
  # Number of initial non-trial frames used to establish a baseline "dark" screen brightness.
  ROI_BRIGHTNESS_BASELINE_FRAMES: 50
  # A stimulus is detected if the ROI brightness > (baseline * this factor).
  ROI_BRIGHTNESS_THRESHOLD_FACTOR: 1.5
  # (Alternative) Use a fixed brightness threshold (0-255). Less robust than the factor method.
  # ROI_ABSOLUTE_BRIGHTNESS_THRESHOLD: 180

  # --- Trial Timing ---
  # How long the stimulus is considered "on" after initial detection.
  STIMULUS_DURATION_MS: 800
  # How long the trial continues *after* the stimulus turns off.
  POST_STIMULUS_TRIAL_DURATION_MS: 0
  # Minimum time required between the end of one trial and the start of the next.
  MIN_INTER_TRIAL_INTERVAL_MS: 1000

  # --- Gaze Classification (During Stimulus Period) ---
  # Method to determine if the user is looking at the stimulus.
  # "eye_gaze_with_head_filter": (RECOMMENDED) Uses iris position but filters out frames
  #                                where the head is turned too far away. Most accurate.
  # "eye_gaze_only": Uses only the relative position of the iris in the eye socket.
  # "head_pose_only": Legacy method. Uses only the calibrated head pose angles.
  GAZE_CLASSIFICATION_METHOD: "head_pose_only"

  # Minimum percentage of the STIMULUS_DURATION_MS that gaze must be "on stimulus"
  # for the entire trial to be classified as a "Looked (1)".
  LOOK_TO_STIMULUS_THRESHOLD_PERCENT: 11

  # -- Parameters for "eye_gaze" methods --
  # Define the "on stimulus" iris position ranges (in pixels, relative to eye corner).
  # YOU MUST TUNE THESE FOR YOUR SPECIFIC SETUP.
  STIMULUS_LEFT_IRIS_DX_RANGE: [-20, -10]
  STIMULUS_LEFT_IRIS_DY_RANGE: [0, 5]
  STIMULUS_RIGHT_IRIS_DX_RANGE: [10, 20]
  STIMULUS_RIGHT_IRIS_DY_RANGE: [-5, 0]

  # -- Parameters for the "head_filter" --
  # Defines the allowable head pose range (in degrees) for eye gaze to be considered valid.
  HEAD_POSE_FILTER_PITCH_RANGE: [-15, 15]
  HEAD_POSE_FILTER_YAW_RANGE: [-15, 15]

  # -- Parameters for "head_pose_only" method --
  # Defines the "on stimulus" head pose range (in degrees from calibrated neutral).
  STIMULUS_PITCH_RANGE: [-10, 10]
  STIMULUS_YAW_RANGE: [-10, 10]

  # -- Gaze Refinements --
  # Thresholds to detect an explicit downward look, which helps prevent
  # counting a downward glance as a blink. Tune these based on your subject's data.
  DOWNWARD_LOOK_LEFT_IRIS_DY_MIN: 10
  DOWNWARD_LOOK_RIGHT_IRIS_DY_MIN: 10

#--------------------------------------------------------------------------------------
# 6. Landmark Indices (Advanced)
# These map MediaPipe's output to named facial features.
# It is STRONGLY recommended NOT to change these unless you are an expert
# and MediaPipe has updated its model.
#--------------------------------------------------------------------------------------
Landmarks:
  LEFT_EYE_IRIS: [474, 475, 476, 477]
  RIGHT_EYE_IRIS: [469, 470, 471, 472]
  RIGHT_EYE_POINTS: [33, 160, 159, 158, 133, 153, 145, 144]
  LEFT_EYE_POINTS: [362, 385, 386, 387, 263, 373, 374, 380]
  RIGHT_EYE_OUTER_CORNER: 33
  LEFT_EYE_OUTER_CORNER: 263
  NOSE_TIP_INDEX: 4
  CHIN_INDEX: 152
  RIGHT_MOUTH_CORNER: 61
  LEFT_MOUTH_CORNER: 291
  # Indices used for the solvePnP head pose algorithm.
  _indices_pose: [4, 152, 263, 33, 291, 61] # Nose, Chin, L-Eye, R-Eye, L-Mouth, R-Mouth
